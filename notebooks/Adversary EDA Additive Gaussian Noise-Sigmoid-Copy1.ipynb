{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import parse\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dionysus as dion\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "from grakel import GraphKernel\n",
    "import grakel\n",
    "import seaborn as sns\n",
    "\n",
    "from pt_activation.models.simple_mnist_sigmoid import CFF\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_info(filename):\n",
    "    format_string = 'true-{}_adv-{}_sample-{}.npy'\n",
    "    parsed = parse.parse(format_string, filename)\n",
    "    return {'true class':int(parsed[0]), 'adv class':int(parsed[1]), 'sample':int(parsed[2])}\n",
    "\n",
    "def read_adversaries(loc):\n",
    "    ret = []\n",
    "    for f in os.listdir(loc):\n",
    "        if os.path.isfile(os.path.join(loc,f)) and f.find('.npy') != -1:\n",
    "            adv = np.load(os.path.join(loc, f))\n",
    "            info = get_adv_info(f)\n",
    "            info['adversary'] = adv\n",
    "            ret.append(info)\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_directory_loc = '/home/tgebhart/projects/pt_activation/logdir/adversaries/additive_gaussian_noise/cff_3-filters_8-kernel_size_50-fc1_sigmoid-activation.pt/90'\n",
    "adversaries = read_adversaries(adv_directory_loc)\n",
    "adversaries = sorted(adversaries,  key=lambda k: k['sample'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def create_filtrations(model, batch_size, up_to):\n",
    "    device = torch.device(\"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])), batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for s in range(data.shape[0]):\n",
    "                row = {'loss':output.cpu().numpy()[s][0], 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0]}\n",
    "                res_df.append(row)\n",
    "\n",
    "            t += batch_size\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df)\n",
    "\n",
    "\n",
    "def create_adversary_filtrations(model, batch_size, up_to, adversaries):\n",
    "    device = torch.device(\"cpu\")\n",
    "    adv_images = torch.tensor(np.array([a['adversary'] for a in adversaries]))\n",
    "    adv_labels = torch.tensor(np.array([a['true class'] for a in adversaries]))\n",
    "    adv_samples = [a['sample'] for a in adversaries]\n",
    "    \n",
    "    print(adv_images.shape, adv_labels.shape)\n",
    "    \n",
    "    advs = torch.utils.data.TensorDataset(adv_images, adv_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(advs, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for s in range(data.shape[0]):\n",
    "                row = {'loss':output.cpu().numpy()[s][0], 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0], 'sample':adv_samples[s+t]}\n",
    "                res_df.append(row)\n",
    "\n",
    "            t += (batch_size)\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filtrations(model, batch_size, up_to):\n",
    "    device = torch.device(\"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])), batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for s in range(data.shape[0]):\n",
    "                # check if this makes sense\n",
    "                this_hiddens = [hiddens[0][s], hiddens[1][s], hiddens[2][s]]\n",
    "                print('Filtration: {}'.format(s+t))\n",
    "                f = model.compute_dynamic_filtration(data[s,0], this_hiddens)\n",
    "                #\n",
    "                row = {'filtration':f, 'loss':output.cpu().numpy()[s][0], 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0]}\n",
    "                res_df.append(row)\n",
    "\n",
    "            t += batch_size\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df)\n",
    "\n",
    "\n",
    "def create_adversary_filtrations(model, batch_size, up_to, adversaries):\n",
    "    device = torch.device(\"cpu\")\n",
    "    adv_images = torch.tensor(np.array([a['adversary'] for a in adversaries]))\n",
    "    adv_labels = torch.tensor(np.array([a['true class'] for a in adversaries]))\n",
    "    \n",
    "    print(adv_images.shape, adv_labels.shape)\n",
    "    \n",
    "    advs = torch.utils.data.TensorDataset(adv_images, adv_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(advs, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for s in range(data.shape[0]):\n",
    "                # check if this makes sense\n",
    "                this_hiddens = [hiddens[0][s], hiddens[1][s], hiddens[2][s]]\n",
    "                print('Filtration: {}'.format(s+t))\n",
    "                f = model.compute_dynamic_filtration(data[s,0], this_hiddens)\n",
    "                #\n",
    "                row = {'filtration':f, 'loss':output.cpu().numpy()[s][0], 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0]}\n",
    "                res_df.append(row)\n",
    "\n",
    "            t += (batch_size)\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = '/home/tgebhart/projects/pt_activation/logdir/models/cff_3-filters_8-kernel_size_50-fc1_sigmoid-activation.pt'\n",
    "model = CFF()\n",
    "model.load_state_dict(torch.load(model_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtration: 0\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 87973\n",
      "Sorting filtration...\n",
      "Filtration: 1\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 95357\n",
      "Sorting filtration...\n",
      "Filtration: 2\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 79401\n",
      "Sorting filtration...\n",
      "Filtration: 3\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 102834\n",
      "Sorting filtration...\n",
      "Filtration: 4\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 90089\n",
      "Sorting filtration...\n",
      "Filtration: 5\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 82683\n",
      "Sorting filtration...\n",
      "Filtration: 6\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 91718\n",
      "Sorting filtration...\n",
      "Filtration: 7\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 91370\n",
      "Sorting filtration...\n",
      "Filtration: 8\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 98219\n",
      "Sorting filtration...\n",
      "Filtration: 9\n",
      "h1_id_start 784\n",
      "h2_id_start 2107\n",
      "h3_id_start 2157\n",
      "filtration size 99169\n",
      "Sorting filtration...\n"
     ]
    }
   ],
   "source": [
    "res_df = create_filtrations(model, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adv_df = create_adversary_filtrations(model, 50, 1000, adversaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_df = create_filtrations(model, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_graphs(df):\n",
    "    sample_graphs = []\n",
    "    for s in range(df.shape[0]):\n",
    "        print(s)\n",
    "        subgraphs = {}\n",
    "        f = df['filtration'].iloc[s]\n",
    "        m = dion.homology_persistence(f)\n",
    "        dgms = dion.init_diagrams(m,f)\n",
    "        for i,c in enumerate(m):\n",
    "            if len(c) == 2:\n",
    "                if f[c[0].index][0] in subgraphs:\n",
    "                    subgraphs[f[c[0].index][0]].add_edge(f[c[0].index][0],f[c[1].index][0],weight=f[i].data)\n",
    "                else:\n",
    "                    eaten = False\n",
    "                    for k, v in subgraphs.items():\n",
    "                        if v.has_node(f[c[0].index][0]):\n",
    "                            v.add_edge(f[c[0].index][0], f[c[1].index][0], weight=f[i].data)\n",
    "                            eaten = True\n",
    "                            break\n",
    "                    if not eaten:\n",
    "                        g = nx.Graph()\n",
    "                        g.add_edge(f[c[0].index][0], f[c[1].index][0], weight=f[i].data)\n",
    "                        subgraphs[f[c[0].index][0]] = g\n",
    "\n",
    "        sample_graphs.append(subgraphs)\n",
    "    return sample_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "sample_graphs = create_sample_graphs(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adv_sample_graphs = create_sample_graphs(adv_df)\n",
    "# with open(os.path.join(adv_directory_loc, 'adv_samples.pkl'), 'wb') as f:\n",
    "#     pickle.dump(adv_sample_graphs, f)\n",
    "# sample_graphs = create_sample_graphs(res_df)\n",
    "# with open(os.path.join(adv_directory_loc, 'samples.pkl'), 'wb') as f:\n",
    "#     pickle.dump(sample_graphs, f)\n",
    "\n",
    "# sample_graphs = pickle.load( open(os.path.join('/home/tgebhart/projects/pt_activation/logdir/adversaries/lbfgsm/cff_3-filters_8-kernel_size_50-fc1_sigmoid-activation.pt/90', 'samples.pkl'), \"rb\") )\n",
    "# adv_sample_graphs = pickle.load( open(os.path.join(adv_directory_loc, 'adv_samples.pkl'), \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = {\n",
    "#     'node_color': 'red',\n",
    "#     'node_size': 2,\n",
    "#     'width': 3,\n",
    "#     'with_labels':True}\n",
    "# nx.draw_random(subgraphs[243], **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrects = res_df[res_df['class'] != res_df['prediction']]\n",
    "corrects = res_df[res_df['class'] == res_df['prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrects.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_list = list(corrects.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_correct_list = list(adv_df[~np.isin(adv_df['sample'], list(incorrects.index))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_graphs = [sample_graphs[i] for i in correct_list]\n",
    "adv_sample_graphs = [adv_sample_graphs[i] for i in adv_correct_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = ['#12355b', '#ff6978']\n",
    "EDGE_COLOR = '#272d2d'\n",
    "PLT_LABELS = ['Unaltered', 'Adversarial']\n",
    "# COLORS = ['#bfdce7', '#ff6978']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sgl = np.zeros(len(sample_graphs))\n",
    "for i in range(len(sample_graphs)):\n",
    "    sgl[i] = len(sample_graphs[i])\n",
    "\n",
    "adv_sgl = np.zeros(len(adv_sample_graphs))\n",
    "for i in range(len(adv_sample_graphs)):\n",
    "    adv_sgl[i] = len(adv_sample_graphs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([sgl, adv_sgl], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Generator Count')\n",
    "plt.xlabel('Number of Generators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "take = 10\n",
    "edges = set()\n",
    "for i in range(len(sample_graphs)):\n",
    "    for k in list(sample_graphs[i].keys())[:take]:\n",
    "        for x in sample_graphs[i][k].edges(data=True):\n",
    "            edge_name = str(x[0])+'-'+str(x[1])\n",
    "            edges.add(edge_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(adv_sample_graphs)):\n",
    "    for k in list(adv_sample_graphs[i].keys())[:take]:\n",
    "        for x in adv_sample_graphs[i][k].edges(data=True):\n",
    "            edge_name = str(x[0])+'-'+str(x[1])\n",
    "            edges.add(edge_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "edf = pd.DataFrame(np.zeros((len(sample_graphs)+len(adv_sample_graphs),len(edges))), columns=list(edges))\n",
    "for i in range(len(sample_graphs)):\n",
    "    print('Sample: {}/{}'.format(i,len(sample_graphs)))\n",
    "    for k in list(sample_graphs[i].keys())[:take]:\n",
    "        for x in sample_graphs[i][k].edges(data=True):\n",
    "            edge_name = str(x[0])+'-'+str(x[1])\n",
    "            edf.iloc[i][edge_name] = x[2]['weight']\n",
    "            \n",
    "for i in range(len(adv_sample_graphs)):\n",
    "    print('Sample: {}/{}'.format(i,len(adv_sample_graphs)))\n",
    "    for k in list(adv_sample_graphs[i].keys())[:take]:\n",
    "        for x in adv_sample_graphs[i][k].edges(data=True):\n",
    "            edge_name = str(x[0])+'-'+str(x[1])\n",
    "            edf.iloc[i+len(sample_graphs)][edge_name] = x[2]['weight']\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "min_max_scaler = sklearn.preprocessing.MinMaxScaler()\n",
    "scaled = min_max_scaler.fit_transform(np.concatenate((sgl, adv_sgl), axis=0).reshape(-1,1))\n",
    "edf['graph counts'] = pd.Series(scaled.T.reshape(-1), index=edf.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = edf.values[:len(sample_graphs)]\n",
    "y = res_df['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC(gamma='scale', decision_function_shape='ovo').fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_adv = edf.values[len(sample_graphs):]\n",
    "adv_preds = clf.predict(X_adv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(adv_df['class'], adv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(adv_df['prediction'], adv_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = svm.SVC(gamma='scale', decision_function_shape='ovo').fit(X[:-100],y[:-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.accuracy_score(y[-100:], clf2.predict(X[-100:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_gois = []\n",
    "for i in range(len(sample_graphs)):\n",
    "    print(i)\n",
    "    a = [sample_graphs[i][k] for k in sample_graphs[i].keys()]\n",
    "    all_gois.append(nx.compose_all(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adv_all_gois = []\n",
    "for i in range(len(adv_sample_graphs)):\n",
    "    print(i)\n",
    "    a = [adv_sample_graphs[i][k] for k in adv_sample_graphs[i].keys()]\n",
    "    adv_all_gois.append(nx.compose_all(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eigs = []\n",
    "for i in range(len(all_gois)):\n",
    "    print('normal ', i)\n",
    "    eigs.append(nx.linalg.laplacian_spectrum(all_gois[i]))\n",
    "\n",
    "adv_eigs = []\n",
    "for i in range(len(adv_all_gois)):\n",
    "    print('adv ', i)\n",
    "    adv_eigs.append(nx.linalg.laplacian_spectrum(adv_all_gois[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_top_eigs = 800\n",
    "top_eigs = np.zeros((len(eigs),num_top_eigs))\n",
    "for i in range(len(eigs)):\n",
    "#     top_eigs[i] = np.sort(eigs[i])[::-1][:num_top_eigs] # descending\n",
    "    top_eigs[i] = np.sort(eigs[i])[:num_top_eigs] # ascending\n",
    "    \n",
    "adv_top_eigs = np.zeros((len(adv_eigs),num_top_eigs))\n",
    "for i in range(len(adv_eigs)):\n",
    "#     adv_top_eigs[i] = np.sort(adv_eigs[i])[::-1][:num_top_eigs] # descending\n",
    "    adv_top_eigs[i] = np.sort(adv_eigs[i])[:num_top_eigs] # ascending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([np.amax(top_eigs, axis=1),np.amax(adv_top_eigs, axis=1)], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Maximum Eigenvalue of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([np.mean(top_eigs, axis=1),np.mean(adv_top_eigs, axis=1)],bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Mean Eigenvalue of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([np.median(top_eigs, axis=1),np.median(adv_top_eigs, axis=1)], range=[0,1.4], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Median Eigenvalue of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([np.amin(top_eigs, axis=1),np.amin(adv_top_eigs, axis=1)], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Minimum Eigenvalue of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([top_eigs[:,1], adv_top_eigs[:,1]], range=[0,0.006], bins='auto',color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Algebraic Connectivity')\n",
    "plt.xlabel('Algebraic Connectivity')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_eigs = np.zeros((len(eigs),3))\n",
    "for i in range(len(eigs)):\n",
    "    mean_eigs[i] = [eigs[i].mean(), eigs[i].shape[0], np.median(eigs[i])]\n",
    "    \n",
    "adv_mean_eigs = np.zeros((len(adv_eigs),3))\n",
    "for i in range(len(adv_eigs)):\n",
    "    adv_mean_eigs[i] = [adv_eigs[i].mean(), adv_eigs[i].shape[0], np.median(adv_eigs[i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([mean_eigs[:,0], adv_mean_eigs[:,0]], range=[2, 7], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Mean Eigenvalues of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([mean_eigs[:,2], adv_mean_eigs[:,2]], range=[0.5,4], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Median Eigenvalues of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([mean_eigs[:,1], adv_mean_eigs[:,1]], range=[1450,1950], bins='auto', color=COLORS, label=PLT_LABELS)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Spectrum Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(adv_mean_eigs[:,1] < 1600)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_num = 10\n",
    "some_gois = []\n",
    "for i in range(len(sample_graphs)):\n",
    "    print(i)\n",
    "    sgik = list(sample_graphs[i].keys())\n",
    "    a = [sample_graphs[i][k] for k in sgik[len(sgik)-some_num:]]\n",
    "    some_gois.append(nx.compose_all(a))\n",
    "    \n",
    "adv_some_gois = []\n",
    "for i in range(len(adv_sample_graphs)):\n",
    "    print(i)\n",
    "    sgik = list(adv_sample_graphs[i].keys())\n",
    "    a = [adv_sample_graphs[i][k] for k in sgik[len(sgik)-some_num:]]\n",
    "    adv_some_gois.append(nx.compose_all(a))\n",
    "    \n",
    "some_eigs = []\n",
    "for i in range(len(some_gois)):\n",
    "    print('normal ', i)\n",
    "    some_eigs.append(nx.linalg.laplacian_spectrum(some_gois[i]))\n",
    "\n",
    "adv_some_eigs = []\n",
    "for i in range(len(adv_some_gois)):\n",
    "    print('adv ', i)\n",
    "    adv_some_eigs.append(nx.linalg.laplacian_spectrum(adv_some_gois[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_mean_eigs = np.zeros((len(some_eigs),5))\n",
    "for i in range(len(some_eigs)):\n",
    "    some_mean_eigs[i] = [some_eigs[i].mean(), some_eigs[i].shape[0], np.median(some_eigs[i]), some_eigs[i].min(), some_eigs[i].max()]\n",
    "    \n",
    "adv_some_mean_eigs = np.zeros((len(adv_some_eigs),5))\n",
    "for i in range(len(adv_some_eigs)):\n",
    "    adv_some_mean_eigs[i] = [adv_some_eigs[i].mean(), adv_some_eigs[i].shape[0], np.median(adv_some_eigs[i]), adv_some_eigs[i].min(), adv_some_eigs[i].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs[:,0], adv_some_mean_eigs[:,0]], range=[0, 3.5], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Mean Eigenvalues of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs[:,2], adv_some_mean_eigs[:,2]], bins='auto', color=COLORS, label=PLT_LABELS)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Median Eigenvalues of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs[:,1], adv_some_mean_eigs[:,1]], range=[0, 300], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Spectrum Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs[:,3], adv_some_mean_eigs[:,3]], range=[-2e-15, 0], bins='auto', color=COLORS, label=PLT_LABELS)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Minimum Eigenvalue of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs[:,4], adv_some_mean_eigs[:,4]], range=[0,50], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Maximum Eigenvalue of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'node_color': COLORS[0],\n",
    "    'node_size': 2,\n",
    "    'width': 2,\n",
    "    'with_labels':False}\n",
    "nx.draw_spring(some_gois[2], **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'node_color': COLORS[1],\n",
    "    'node_size': 2,\n",
    "    'width': 2,\n",
    "    'with_labels':False}\n",
    "nx.draw_spring(adv_some_gois[2], **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_num = 10\n",
    "some_gois2 = []\n",
    "for i in range(len(sample_graphs)):\n",
    "    print(i)\n",
    "    sgik = list(sample_graphs[i].keys())\n",
    "    a = [sample_graphs[i][k] for k in sgik[:some_num]]\n",
    "    some_gois2.append(nx.compose_all(a))\n",
    "    \n",
    "adv_some_gois2 = []\n",
    "for i in range(len(adv_sample_graphs)):\n",
    "    print(i)\n",
    "    sgik = list(adv_sample_graphs[i].keys())\n",
    "    a = [adv_sample_graphs[i][k] for k in sgik[:some_num]]\n",
    "    adv_some_gois2.append(nx.compose_all(a))\n",
    "    \n",
    "some_eigs2 = []\n",
    "for i in range(len(some_gois)):\n",
    "    print('normal ', i)\n",
    "    some_eigs2.append(nx.linalg.laplacian_spectrum(some_gois2[i]))\n",
    "\n",
    "adv_some_eigs2 = []\n",
    "for i in range(len(adv_some_gois)):\n",
    "    print('adv ', i)\n",
    "    adv_some_eigs2.append(nx.linalg.laplacian_spectrum(adv_some_gois2[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "some_mean_eigs2 = np.zeros((len(some_eigs2),5))\n",
    "for i in range(len(some_eigs)):\n",
    "    some_mean_eigs2[i] = [some_eigs2[i].mean(), some_eigs2[i].shape[0], np.median(some_eigs2[i]), some_eigs2[i].min(), some_eigs2[i].max()]\n",
    "    \n",
    "adv_some_mean_eigs2 = np.zeros((len(adv_some_eigs2),5))\n",
    "for i in range(len(adv_some_eigs2)):\n",
    "    adv_some_mean_eigs2[i] = [adv_some_eigs2[i].mean(), adv_some_eigs2[i].shape[0], np.median(adv_some_eigs2[i]), adv_some_eigs2[i].min(), adv_some_eigs2[i].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs2[:,0], adv_some_mean_eigs2[:,0]], range=[2, 8], bins='auto', color=COLORS, label=PLT_LABELS)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Mean Eigenvalues of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs2[:,2], adv_some_mean_eigs2[:,2]], range=[0.5, 5], bins='auto', color=COLORS, label=PLT_LABELS)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Median Eigenvalues of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs2[:,1], adv_some_mean_eigs2[:,1]], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Spectrum Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs2[:,3], adv_some_mean_eigs2[:,3]], bins='auto', color=COLORS, label=PLT_LABELS, edgecolor=EDGE_COLOR)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Minimum Eigenvalue of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([some_mean_eigs2[:,4], adv_some_mean_eigs2[:,4]], range=[40, 300], bins='auto', color=COLORS, label=PLT_LABELS)\n",
    "plt.legend()\n",
    "plt.title('Distribution of Maximum Eigenvalue of Spectrum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(adversaries[5]['adversary'].reshape(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'node_color': COLORS[0],\n",
    "    'node_size': 2,\n",
    "    'width': 2,\n",
    "    'with_labels':False}\n",
    "nx.draw_spring(some_gois2[2], **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'node_color': COLORS[1],\n",
    "    'node_size': 2,\n",
    "    'width': 2,\n",
    "    'with_labels':False}\n",
    "nx.draw_spring(adv_some_gois2[2], **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weiners = np.zeros(len(all_gois))\n",
    "for i in range(len(all_gois)):\n",
    "    print('normal ', i)\n",
    "    weiners[i] = nx.algorithms.wiener.wiener_index(all_gois[i], weight='weight')\n",
    "\n",
    "adv_weiners = np.zeros(len(adv_all_gois))\n",
    "for i in range(len(adv_all_gois)):\n",
    "    print('adv ', i)\n",
    "    adv_weiners[i] = nx.algorithms.wiener.wiener_index(adv_all_gois[i], weight='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist([weiners, adv_weiners], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_df['loss'].max())\n",
    "res_df.iloc[np.where(weiners == .weiners.max())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk = GraphKernel(kernel={\"name\": \"multiscale_laplacian\",\n",
    "                             \"which\": \"fast\",\n",
    "                             \"L\": 1,\n",
    "                             \"P\": 10,\n",
    "                             \"N\": 10})\n",
    "# gk = grakel.RandomWalkLabeled(method_type='fast', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_gois)):\n",
    "    l = list(all_gois[i].nodes())\n",
    "    for j in range(len(l)):\n",
    "        all_gois[i].node[l[j]]['label'] = l[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(adv_all_gois)):\n",
    "    l = list(adv_all_gois[i].nodes())\n",
    "    for j in range(len(l)):\n",
    "        adv_all_gois[i].node[l[j]]['label'] = l[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_adv = grakel.graph_from_networkx(adv_all_gois, node_labels_tag='label', edge_weight_tag='weight')\n",
    "G = grakel.graph_from_networkx(all_gois, node_labels_tag='label', edge_weight_tag='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_train = gk.fit_transform(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
