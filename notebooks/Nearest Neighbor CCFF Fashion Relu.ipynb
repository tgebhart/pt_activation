{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- which subgraphs actually touch the final layer?\n",
    "- which subgraphs actually touch the final layer and contain the correct class as a node in the subgraph?\n",
    "- are nearest neighbors represented as nearest neighbors in graph space?\n",
    "- what are the nearest neighbors of the additional graph structure induced by the adversary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tgebhart/.local/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import parse\n",
    "import umap\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import dionysus as dion\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import hamming, cosine\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from pt_activation.models.ccff import CCFF\n",
    "\n",
    "PERCENTILE = 0\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_info(filename):\n",
    "    format_string = 'true-{}_adv-{}_sample-{}.npy'\n",
    "    parsed = parse.parse(format_string, filename)\n",
    "    return {'true class':int(parsed[0]), 'adv class':int(parsed[1]), 'sample':int(parsed[2])}\n",
    "\n",
    "def read_adversaries(loc):\n",
    "    ret = []\n",
    "    for f in os.listdir(loc):\n",
    "        if os.path.isfile(os.path.join(loc,f)) and f.find('.npy') != -1:\n",
    "            adv = np.load(os.path.join(loc, f))\n",
    "            info = get_adv_info(f)\n",
    "            info['adversary'] = adv\n",
    "            ret.append(info)\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_directory_loc = '/home/tgebhart/projects/pt_activation/logdir/adversaries/fashion/projected_gradient_descent/ccff.pt'\n",
    "adversaries = read_adversaries(adv_directory_loc)\n",
    "adversaries = sorted(adversaries,  key=lambda k: k['sample'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_dists(adversaries, up_to):\n",
    "    device = torch.device('cpu')\n",
    "    dists = np.zeros(up_to)\n",
    "    i = 0\n",
    "    ims = []\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../../data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,))])), batch_size=1, shuffle=False, **kwargs)\n",
    "    adv_images = np.array([a['adversary'] for a in adversaries])\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            d = data.numpy()[0].reshape(28*28)\n",
    "            ims.append(d)\n",
    "            dists[i] = np.linalg.norm(d-adv_images[i].reshape(28*28),2)\n",
    "            i += 1\n",
    "            if i >= up_to:\n",
    "                break\n",
    "            \n",
    "    return dists, ims, adv_images\n",
    "\n",
    "def create_sample_graph(f,tnms,wm):\n",
    "\n",
    "    subgraphs = {}\n",
    "    m = dion.homology_persistence(f)\n",
    "    dgm = dion.init_diagrams(m,f)[0]\n",
    "    for i,c in enumerate(m):\n",
    "        if len(c) == 2:\n",
    "            w = f[i].data\n",
    "            if (tnms[f[c[0].index][0]],tnms[f[c[1].index][0]]) in wm:\n",
    "                w = wm[(tnms[f[c[0].index][0]],tnms[f[c[1].index][0]])]\n",
    "            elif (tnms[f[c[1].index][0]],tnms[f[c[0].index][0]]) in wm:\n",
    "                w = wm[(tnms[f[c[1].index][0]],tnms[f[c[0].index][0]])]\n",
    "#                 else:\n",
    "#                     print((tnms[f[c[0].index][0]],tnms[f[c[1].index][0]]))\n",
    "#                     raise Exception('NO WM!')\n",
    "            if tnms[f[c[0].index][0]] in subgraphs:\n",
    "                subgraphs[tnms[f[c[0].index][0]]].add_edge(tnms[f[c[0].index][0]],tnms[f[c[1].index][0]], weight=w)\n",
    "            else:\n",
    "                eaten = False\n",
    "                for k, v in subgraphs.items():\n",
    "                    if v.has_node(tnms[f[c[0].index][0]]):\n",
    "                        if tnms[f[c[1].index][0]] in subgraphs:\n",
    "                            v.add_node(tnms[f[c[1].index][0]])\n",
    "#                                 subgraphs[k] = nx.union(v, subgraphs[tnms[f[c[1].index][0]]])\n",
    "                        else:\n",
    "                            v.add_edge(tnms[f[c[0].index][0]], tnms[f[c[1].index][0]], weight=w)\n",
    "                        eaten = True\n",
    "                        break\n",
    "                if not eaten:\n",
    "                    g = nx.Graph()\n",
    "                    g.add_edge(tnms[f[c[0].index][0]], tnms[f[c[1].index][0]], weight=w)\n",
    "                    subgraphs[tnms[f[c[0].index][0]]] = g\n",
    "\n",
    "    return subgraphs, dgm, create_lifetimes(f,subgraphs,dgm,tnms)\n",
    "\n",
    "def create_lifetimes(f, subgraphs, dgm, ids):\n",
    "    lifetimes = {}\n",
    "    for pt in dgm:\n",
    "        k = ids[f[pt.data][0]]\n",
    "        if k in subgraphs.keys():\n",
    "            if pt.death < float('inf'):\n",
    "                lifetimes[k] = pt.birth - pt.death\n",
    "            else:\n",
    "                lifetimes[k] = pt.birth\n",
    "    return lifetimes\n",
    "\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def create_subgraphs(model, batch_size, up_to):\n",
    "    device = torch.device(\"cpu\")\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../../data', train=False, download=True, transform=transforms.Compose([transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5,), (0.5,))])), batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    subgraphs = []\n",
    "    diagrams = []\n",
    "    lifetimes = []\n",
    "    ims = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss = F.nll_loss(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "            for s in range(data.shape[0]):\n",
    "                this_hiddens = [hiddens[i][s] for i in range(len(hiddens))]\n",
    "                print('Filtration: {}'.format(s+t))\n",
    "                f, nm, wm = model.compute_dynamic_filtration2(data[s], this_hiddens, percentile=PERCENTILE, return_nm=True, absolute_value=True)\n",
    "                tnm = {v: k for k, v in nm.items()}\n",
    "                sg, dg, lifetime = create_sample_graph(f, tnm, wm)\n",
    "                row = {'loss':test_loss, 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0]}\n",
    "                res_df.append(row)\n",
    "                subgraphs.append(sg)\n",
    "                diagrams.append(dg)\n",
    "                lifetimes.append(lifetime)\n",
    "                ims.append(data[s].numpy().reshape(28*28))\n",
    "            t += batch_size\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df), subgraphs, diagrams, lifetimes, ims\n",
    "\n",
    "\n",
    "def create_adversary_subgraphs(model, batch_size, up_to, adversaries):\n",
    "    device = torch.device(\"cpu\")\n",
    "    adv_images = torch.tensor(np.array([a['adversary'] for a in adversaries]))\n",
    "    adv_labels = torch.tensor(np.array([a['true class'] for a in adversaries]))\n",
    "    adv_samples = [a['sample'] for a in adversaries]\n",
    "\n",
    "    print(adv_images.shape, adv_labels.shape)\n",
    "\n",
    "    advs = torch.utils.data.TensorDataset(adv_images, adv_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(advs, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    subgraphs = []\n",
    "    diagrams = []\n",
    "    lifetimes = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss = F.nll_loss(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for s in range(data.shape[0]):\n",
    "                this_hiddens = [hiddens[i][s] for i in range(len(hiddens))]\n",
    "                print('Filtration: {}'.format(s+t))\n",
    "                f, nm, wm = model.compute_dynamic_filtration2(data[s], this_hiddens, percentile=PERCENTILE, return_nm=True, absolute_value=True)\n",
    "                tnm = {v: k for k, v in nm.items()}\n",
    "                sg, dg, lifetime = create_sample_graph(f, tnm, wm)\n",
    "                row = {'loss':test_loss, 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0]}\n",
    "                res_df.append(row)\n",
    "                subgraphs.append(sg)\n",
    "                diagrams.append(dg)\n",
    "                lifetimes.append(lifetime)\n",
    "\n",
    "            t += (batch_size)\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df), subgraphs, diagrams, lifetimes, adv_images\n",
    "\n",
    "def create_moved_subgraphs(model, ims, labels, devs, batch_size, up_to, times=1):\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    nims = []\n",
    "    for i in range(len(ims)):\n",
    "#         print('finding alteration number', i)\n",
    "        im = ims[i]\n",
    "        dev = devs[i]\n",
    "        delim = np.zeros(im.shape, dtype=im.dtype)\n",
    "        sigma = 0\n",
    "        while np.linalg.norm(im+delim - im) < dev:\n",
    "            sigma += 0.00001\n",
    "            delim = np.random.normal(scale=sigma, size=im.shape)\n",
    "        nims.append((im+delim).reshape(1,28,28))\n",
    "    tims = torch.FloatTensor(nims)\n",
    "    tlabels = torch.tensor(labels)\n",
    "    print(tims.shape, tlabels.shape, type(tims), type(tlabels))\n",
    "    dataset = torch.utils.data.TensorDataset(tims, tlabels)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    subgraphs = []\n",
    "    diagrams = []\n",
    "    lifetimes = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss = F.nll_loss(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for s in range(data.shape[0]):\n",
    "                this_hiddens = [hiddens[i][s] for i in range(len(hiddens))]\n",
    "                print('Filtration: {}'.format(s+t))\n",
    "                f, nm, wm = model.compute_dynamic_filtration2(data[s], this_hiddens, percentile=PERCENTILE, return_nm=True, absolute_value=True)\n",
    "                tnm = {v: k for k, v in nm.items()}\n",
    "                sg, dg, lifetime = create_sample_graph(f, tnm, wm)\n",
    "                row = {'loss':test_loss, 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0]}\n",
    "                res_df.append(row)\n",
    "                subgraphs.append(sg)\n",
    "                diagrams.append(dg)\n",
    "                lifetimes.append(lifetime)\n",
    "\n",
    "            t += (batch_size)\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df), subgraphs, diagrams, lifetimes, nims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_location = '/home/tgebhart/projects/pt_activation/logdir/models/fashion/ccff.pt'\n",
    "model = CCFF()\n",
    "model.load_state_dict(torch.load(model_location, map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtration: 0\n",
      "filtration size 228117\n",
      "Filtration: 1\n",
      "filtration size 342448\n",
      "Filtration: 2\n",
      "filtration size 236944\n",
      "Filtration: 3\n",
      "filtration size 191577\n",
      "Filtration: 4\n",
      "filtration size 288580\n",
      "Filtration: 5\n",
      "filtration size 253254\n",
      "Filtration: 6\n",
      "filtration size 258888\n",
      "Filtration: 7\n",
      "filtration size 299131\n",
      "Filtration: 8\n",
      "filtration size 138141\n",
      "Filtration: 9\n",
      "filtration size 209745\n",
      "Filtration: 10\n",
      "filtration size 319230\n",
      "Filtration: 11\n",
      "filtration size 216614\n",
      "Filtration: 12\n",
      "filtration size 188520\n",
      "Filtration: 13\n",
      "filtration size 214674\n",
      "Filtration: 14\n",
      "filtration size 362447\n",
      "Filtration: 15\n",
      "filtration size 198798\n",
      "Filtration: 16\n",
      "filtration size 295903\n",
      "Filtration: 17\n",
      "filtration size 316856\n",
      "Filtration: 18\n",
      "filtration size 307589\n",
      "Filtration: 19\n",
      "filtration size 328125\n",
      "Filtration: 20\n",
      "filtration size 346280\n",
      "Filtration: 21\n",
      "filtration size 212683\n",
      "Filtration: 22\n",
      "filtration size 234097\n",
      "Filtration: 23\n",
      "filtration size 238177\n",
      "Filtration: 24\n",
      "filtration size 205104\n",
      "Filtration: 25\n",
      "filtration size 253470\n",
      "Filtration: 26\n",
      "filtration size 332714\n",
      "Filtration: 27\n",
      "filtration size 298759\n",
      "Filtration: 28\n",
      "filtration size 271383\n",
      "Filtration: 29\n",
      "filtration size 308328\n",
      "Filtration: 30\n",
      "filtration size 328800\n",
      "Filtration: 31\n",
      "filtration size 237500\n",
      "Filtration: 32\n",
      "filtration size 153469\n",
      "Filtration: 33\n",
      "filtration size 308084\n",
      "Filtration: 34\n",
      "filtration size 290410\n",
      "Filtration: 35\n",
      "filtration size 260916\n",
      "Filtration: 36\n",
      "filtration size 229289\n",
      "Filtration: 37\n",
      "filtration size 144002\n",
      "Filtration: 38\n",
      "filtration size 239520\n",
      "Filtration: 39\n",
      "filtration size 260758\n",
      "Filtration: 40\n",
      "filtration size 321009\n",
      "Filtration: 41\n",
      "filtration size 229232\n",
      "Filtration: 42\n",
      "filtration size 306153\n",
      "Filtration: 43\n",
      "filtration size 250793\n",
      "Filtration: 44\n",
      "filtration size 363120\n",
      "Filtration: 45\n",
      "filtration size 214466\n",
      "Filtration: 46\n",
      "filtration size 371876\n",
      "Filtration: 47\n",
      "filtration size 230550\n",
      "Filtration: 48\n",
      "filtration size 271098\n",
      "Filtration: 49\n",
      "filtration size 352373\n",
      "Filtration: 50\n",
      "filtration size 355006\n",
      "Filtration: 51\n",
      "filtration size 279937\n",
      "Filtration: 52\n",
      "filtration size 181769\n",
      "Filtration: 53\n",
      "filtration size 381276\n",
      "Filtration: 54\n",
      "filtration size 279091\n",
      "Filtration: 55\n",
      "filtration size 358592\n",
      "Filtration: 56\n",
      "filtration size 336928\n",
      "Filtration: 57\n",
      "filtration size 321836\n",
      "Filtration: 58\n",
      "filtration size 305645\n",
      "Filtration: 59\n",
      "filtration size 214133\n",
      "Filtration: 60\n",
      "filtration size 202052\n",
      "Filtration: 61\n",
      "filtration size 224679\n",
      "Filtration: 62\n",
      "filtration size 289623\n",
      "Filtration: 63\n",
      "filtration size 217284\n",
      "Filtration: 64\n",
      "filtration size 245479\n",
      "Filtration: 65\n",
      "filtration size 182605\n",
      "Filtration: 66\n",
      "filtration size 234350\n",
      "Filtration: 67\n",
      "filtration size 270875\n",
      "Filtration: 68\n",
      "filtration size 242712\n",
      "Filtration: 69\n",
      "filtration size 329023\n",
      "Filtration: 70\n",
      "filtration size 230850\n",
      "Filtration: 71\n",
      "filtration size 234546\n",
      "Filtration: 72\n",
      "filtration size 377911\n",
      "Filtration: 73\n",
      "filtration size 339643\n",
      "Filtration: 74\n",
      "filtration size 324967\n",
      "Filtration: 75\n",
      "filtration size 178007\n",
      "Filtration: 76\n",
      "filtration size 236685\n",
      "Filtration: 77\n",
      "filtration size 362295\n",
      "Filtration: 78\n",
      "filtration size 306652\n",
      "Filtration: 79\n",
      "filtration size 364300\n",
      "Filtration: 80\n",
      "filtration size 206320\n",
      "Filtration: 81\n",
      "filtration size 340186\n",
      "Filtration: 82\n",
      "filtration size 249704\n",
      "Filtration: 83\n",
      "filtration size 272558\n",
      "Filtration: 84\n",
      "filtration size 195505\n",
      "Filtration: 85\n",
      "filtration size 324849\n",
      "Filtration: 86\n",
      "filtration size 261083\n",
      "Filtration: 87\n",
      "filtration size 350680\n",
      "Filtration: 88\n",
      "filtration size 317858\n",
      "Filtration: 89\n",
      "filtration size 367866\n",
      "Filtration: 90\n",
      "filtration size 157147\n",
      "Filtration: 91\n",
      "filtration size 185036\n",
      "Filtration: 92\n",
      "filtration size 357380\n",
      "Filtration: 93\n",
      "filtration size 235140\n",
      "Filtration: 94\n",
      "filtration size 212569\n",
      "Filtration: 95\n",
      "filtration size 309667\n",
      "Filtration: 96\n",
      "filtration size 255929\n",
      "Filtration: 97\n",
      "filtration size 216204\n",
      "Filtration: 98\n",
      "filtration size 363920\n",
      "Filtration: 99\n",
      "filtration size 349700\n",
      "Filtration: 100\n",
      "filtration size 215398\n",
      "Filtration: 101\n",
      "filtration size 313528\n",
      "Filtration: 102\n",
      "filtration size 201809\n",
      "Filtration: 103\n",
      "filtration size 302629\n",
      "Filtration: 104\n",
      "filtration size 229536\n",
      "Filtration: 105\n",
      "filtration size 266323\n",
      "Filtration: 106\n",
      "filtration size 160940\n",
      "Filtration: 107\n",
      "filtration size 250056\n",
      "Filtration: 108\n",
      "filtration size 239029\n",
      "Filtration: 109\n",
      "filtration size 333201\n",
      "Filtration: 110\n",
      "filtration size 268737\n",
      "Filtration: 111\n",
      "filtration size 163060\n",
      "Filtration: 112\n",
      "filtration size 195578\n",
      "Filtration: 113\n",
      "filtration size 332989\n",
      "Filtration: 114\n",
      "filtration size 190807\n",
      "Filtration: 115\n",
      "filtration size 345962\n",
      "Filtration: 116\n",
      "filtration size 312776\n",
      "Filtration: 117\n",
      "filtration size 296991\n",
      "Filtration: 118\n",
      "filtration size 204990\n",
      "Filtration: 119\n",
      "filtration size 373936\n",
      "Filtration: 120\n",
      "filtration size 302871\n",
      "Filtration: 121\n",
      "filtration size 311463\n",
      "Filtration: 122\n",
      "filtration size 232467\n",
      "Filtration: 123\n",
      "filtration size 248053\n",
      "Filtration: 124\n",
      "filtration size 301537\n",
      "Filtration: 125\n",
      "filtration size 326224\n",
      "Filtration: 126\n",
      "filtration size 295767\n",
      "Filtration: 127\n",
      "filtration size 345908\n",
      "Filtration: 128\n",
      "filtration size 225785\n",
      "Filtration: 129\n",
      "filtration size 176252\n",
      "Filtration: 130\n",
      "filtration size 345784\n",
      "Filtration: 131\n",
      "filtration size 194766\n",
      "Filtration: 132\n",
      "filtration size 265802\n",
      "Filtration: 133\n",
      "filtration size 237001\n",
      "Filtration: 134\n",
      "filtration size 361548\n",
      "Filtration: 135\n",
      "filtration size 263208\n",
      "Filtration: 136\n",
      "filtration size 337560\n",
      "Filtration: 137\n",
      "filtration size 211846\n",
      "Filtration: 138\n",
      "filtration size 287527\n",
      "Filtration: 139\n",
      "filtration size 209676\n",
      "Filtration: 140\n",
      "filtration size 234374\n",
      "Filtration: 141\n",
      "filtration size 363095\n",
      "Filtration: 142\n",
      "filtration size 240304\n",
      "Filtration: 143\n",
      "filtration size 323627\n",
      "Filtration: 144\n",
      "filtration size 206922\n",
      "Filtration: 145\n",
      "filtration size 369585\n",
      "Filtration: 146\n",
      "filtration size 196349\n",
      "Filtration: 147\n",
      "filtration size 238264\n",
      "Filtration: 148\n",
      "filtration size 369245\n",
      "Filtration: 149\n",
      "filtration size 348308\n",
      "Filtration: 150\n",
      "filtration size 281945\n",
      "Filtration: 151\n",
      "filtration size 323451\n",
      "Filtration: 152\n",
      "filtration size 233373\n",
      "Filtration: 153\n",
      "filtration size 328384\n",
      "Filtration: 154\n",
      "filtration size 145722\n",
      "Filtration: 155\n",
      "filtration size 361028\n",
      "Filtration: 156\n",
      "filtration size 328214\n",
      "Filtration: 157\n",
      "filtration size 190739\n",
      "Filtration: 158\n",
      "filtration size 255424\n",
      "Filtration: 159\n",
      "filtration size 257841\n",
      "Filtration: 160\n",
      "filtration size 243816\n",
      "Filtration: 161\n",
      "filtration size 323739\n",
      "Filtration: 162\n",
      "filtration size 266974\n",
      "Filtration: 163\n",
      "filtration size 219960\n",
      "Filtration: 164\n",
      "filtration size 257155\n",
      "Filtration: 165\n",
      "filtration size 261512\n",
      "Filtration: 166\n",
      "filtration size 334388\n",
      "Filtration: 167\n",
      "filtration size 249420\n",
      "Filtration: 168\n",
      "filtration size 358167\n",
      "Filtration: 169\n",
      "filtration size 378748\n",
      "Filtration: 170\n",
      "filtration size 283375\n",
      "Filtration: 171\n",
      "filtration size 178754\n",
      "Filtration: 172\n",
      "filtration size 348373\n",
      "Filtration: 173\n",
      "filtration size 185373\n",
      "Filtration: 174\n",
      "filtration size 364072\n",
      "Filtration: 175\n",
      "filtration size 297085\n",
      "Filtration: 176\n",
      "filtration size 249954\n",
      "Filtration: 177\n",
      "filtration size 346852\n",
      "Filtration: 178\n",
      "filtration size 231110\n",
      "Filtration: 179\n",
      "filtration size 215003\n",
      "Filtration: 180\n",
      "filtration size 310915\n",
      "Filtration: 181\n",
      "filtration size 257526\n",
      "Filtration: 182\n",
      "filtration size 347736\n",
      "Filtration: 183\n",
      "filtration size 330499\n",
      "Filtration: 184\n",
      "filtration size 192650\n",
      "Filtration: 185\n",
      "filtration size 270017\n",
      "Filtration: 186\n",
      "filtration size 266247\n",
      "Filtration: 187\n",
      "filtration size 356311\n",
      "Filtration: 188\n",
      "filtration size 358792\n",
      "Filtration: 189\n",
      "filtration size 195082\n",
      "Filtration: 190\n",
      "filtration size 274775\n",
      "Filtration: 191\n",
      "filtration size 250294\n",
      "Filtration: 192\n",
      "filtration size 259774\n",
      "Filtration: 193\n",
      "filtration size 359689\n",
      "Filtration: 194\n",
      "filtration size 191910\n",
      "Filtration: 195\n",
      "filtration size 349210\n",
      "Filtration: 196\n",
      "filtration size 283718\n",
      "Filtration: 197\n",
      "filtration size 289481\n",
      "Filtration: 198\n",
      "filtration size 270113\n",
      "Filtration: 199\n",
      "filtration size 225299\n"
     ]
    }
   ],
   "source": [
    "res_df, sample_graphs, dgms, lifetimes, ims = create_subgraphs(model, 50, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adv_df, adv_nms, adv_wms = create_adversary_filtrations(model, 50, 100, adversaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nearest_neighbors(ims):\n",
    "    ret = np.zeros((ims.shape[0],ims.shape[0]))\n",
    "    for i in range(ims.shape[0]):\n",
    "        for j in range(ims.shape[0]):\n",
    "#             nbrs = NearestNeighbors(n_neighbors=10).fit(ret_ims)\n",
    "#             ret = nbrs.kneighbors(ret_ims)\n",
    "#             for i in range(data.shape[0]):\n",
    "#                 for j in range(data.shape[0]):\n",
    "            ret[i,j] = np.linalg.norm(ims[i] - ims[j], 0)\n",
    "                \n",
    "    return ret\n",
    "\n",
    "def nearest_neighbors_umap(ims, n_neighbors=15, min_dist=0.1, n_components=10, metric='euclidean'):\n",
    "    ret = np.zeros((ims.shape[0], ims.shape[0]))\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        metric=metric\n",
    "    )\n",
    "    embedding = reducer.fit_transform(ims)\n",
    "    for i in range(embedding.shape[0]):\n",
    "        for j in range(embedding.shape[0]):\n",
    "            ret[i,j] = 1/(1+np.linalg.norm(embedding[i] - embedding[j]))\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nns = nearest_neighbors_umap(np.array([ims[i].reshape(28*28) for i in range(len(ims))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>loss</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class      loss  prediction\n",
       "0      9  0.619969           9\n",
       "1      2  0.619969           2\n",
       "2      1  0.619969           1\n",
       "3      1  0.619969           1\n",
       "4      6  0.619969           6"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_distance(g1, g2, ret_labels=False):\n",
    "    nodeset = set(list(g1.nodes) + list(g2.nodes))\n",
    "    g1_vec = np.zeros((len(nodeset)))\n",
    "    g2_vec = np.zeros((len(nodeset)))\n",
    "    nodesetlist = list(nodeset)\n",
    "    for i in range(len(nodesetlist)):\n",
    "        node = nodesetlist[i]\n",
    "        if node in g1.nodes:\n",
    "            g1_vec[i] = 1.0\n",
    "        if node in g2.nodes:\n",
    "            g2_vec[i] = 1.0\n",
    "    if ret_labels:\n",
    "        return hamming(g1_vec, g2_vec), nodesetlist\n",
    "    else:\n",
    "        return hamming(g1_vec, g2_vec)\n",
    "    \n",
    "def edge_hamming_distance(g1, g2, ret_labels=False):\n",
    "    edgeset = set(list(g1.edges) + list(g2.edges))\n",
    "    g1_vec = np.zeros((len(edgeset)))\n",
    "    g2_vec = np.zeros((len(edgeset)))\n",
    "    edgesetlist = list(edgeset)\n",
    "    for i in range(len(edgesetlist)):\n",
    "        edge = edgesetlist[i]\n",
    "        if edge in g1.edges:\n",
    "            g1_vec[i] = 1.0\n",
    "        if edge in g2.edges:\n",
    "            g2_vec[i] = 1.0\n",
    "    if ret_labels:\n",
    "        return hamming(g1_vec, g2_vec), edgesetlist\n",
    "    else:\n",
    "        return hamming(g1_vec, g2_vec)\n",
    "    \n",
    "def lifetime_weighted_edge_distance(subgraphs1,subgraphs2,lifetimes1,lifetimes2,ret_labels=False):\n",
    "    edges1 = {}\n",
    "    edges2 = {}\n",
    "    sg1keys = list(subgraphs1.keys())\n",
    "    sg2keys = list(subgraphs2.keys())\n",
    "    lifetimes1 = list(lifetimes1.values())\n",
    "    lifetimes2 = list(lifetimes2.values())\n",
    "    ml1 = max(lifetimes1)\n",
    "    ml2 = max(lifetimes2)\n",
    "    for i in range(len(sg1keys)):\n",
    "        g = subgraphs1[sg1keys[i]]\n",
    "        for e in g.edges:\n",
    "            edges1[e] = lifetimes1[i]/ml1\n",
    "    for i in range(len(sg2keys)):\n",
    "        g = subgraphs2[sg2keys[i]]\n",
    "        for e in g.edges:\n",
    "            edges2[e] = lifetimes2[i]/ml2\n",
    "    edgeset = set(list(edges1.keys()) + list(edges2.keys()))\n",
    "    g1_vec = np.zeros((len(edgeset)))\n",
    "    g2_vec = np.zeros((len(edgeset)))\n",
    "    edgesetlist = list(edgeset)\n",
    "    for i in range(len(edgesetlist)):\n",
    "        edge = edgesetlist[i]\n",
    "        if edge in edges1:\n",
    "            g1_vec[i] += edges1[edge]\n",
    "        if edge in edges2:\n",
    "            g2_vec[i] += edges2[edge]\n",
    "    if ret_labels:\n",
    "        return cosine(g1_vec, g2_vec), edgesetlist\n",
    "    else:\n",
    "        return cosine(g1_vec, g2_vec)\n",
    "    \n",
    "    \n",
    "def full_weighted_edge_distance(subgraphs1, subgraphs2):\n",
    "    edges1 = {}\n",
    "    edges2 = {}\n",
    "    sg1keys = list(subgraphs1.keys())\n",
    "    sg2keys = list(subgraphs2.keys())\n",
    "    lifetimes1 = np.ones(len(subgraphs1))\n",
    "    lifetimes2 = np.ones(len(subgraphs2))\n",
    "    ml1 = max(lifetimes1)\n",
    "    ml2 = max(lifetimes2)\n",
    "    for i in range(len(sg1keys)):\n",
    "        g = subgraphs1[sg1keys[i]]\n",
    "        for e in g.edges:\n",
    "            edges1[e] = lifetimes1[i]/ml1\n",
    "    for i in range(len(sg2keys)):\n",
    "        g = subgraphs2[sg2keys[i]]\n",
    "        for e in g.edges:\n",
    "            edges2[e] = lifetimes2[i]/ml2\n",
    "    edgeset = set(list(edges1.keys()) + list(edges2.keys()))\n",
    "    g1_vec = np.zeros((len(edgeset)))\n",
    "    g2_vec = np.zeros((len(edgeset)))\n",
    "    edgesetlist = list(edgeset)\n",
    "    for i in range(len(edgesetlist)):\n",
    "        edge = edgesetlist[i]\n",
    "        if edge in edges1:\n",
    "            g1_vec[i] += edges1[edge]\n",
    "        if edge in edges2:\n",
    "            g2_vec[i] += edges2[edge]\n",
    "            \n",
    "    return cosine(g1_vec, g2_vec)\n",
    "    \n",
    "def weighted_edge_distance(g1, g2, ret_labels=False):\n",
    "    edgeset = set(list(g1.edges) + list(g2.edges))\n",
    "    g1_vec = np.zeros((len(edgeset)))\n",
    "    g2_vec = np.zeros((len(edgeset)))\n",
    "    edgesetlist = list(edgeset)\n",
    "    for i in range(len(edgesetlist)):\n",
    "        edge = edgesetlist[i]\n",
    "        if edge in g1.edges:\n",
    "            g1_vec[i] = g1[edge[0]][edge[1]]['weight']\n",
    "        if edge in g2.edges:\n",
    "            g2_vec[i] = g2[edge[0]][edge[1]]['weight']\n",
    "    if ret_labels:\n",
    "        return cosine(g1_vec, g2_vec), edgesetlist\n",
    "    else:\n",
    "        return cosine(g1_vec, g2_vec)\n",
    "    \n",
    "def total_edge_weights(graphs):\n",
    "    emap = {}\n",
    "    for g in graphs:\n",
    "        for e in g.edges:\n",
    "            if e in emap:\n",
    "                emap[e] += g[e[0]][e[1]]['weight']\n",
    "            else:\n",
    "                emap[e] = g[e[0]][e[1]]['weight']\n",
    "    return emap\n",
    "\n",
    "def count_nodes(graphs):\n",
    "    nmap = {}\n",
    "    for g in graphs:\n",
    "        for n in g.nodes:\n",
    "            if n in nmap:\n",
    "                nmap[n] += 1.0\n",
    "            else:\n",
    "                nm[2] = 1.0\n",
    "    return nmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>loss</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>0</td>\n",
       "      <td>0.282476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>0</td>\n",
       "      <td>0.282476</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>6</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     class      loss  prediction\n",
       "121      0  0.282476           0\n",
       "143      0  0.282476           0\n",
       "27       0  0.619969           0\n",
       "35       0  0.619969           0\n",
       "40       6  0.619969           0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_res_df = res_df.sort_values(by=['prediction'])\n",
    "sorted_res_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-9843818ef335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_graphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mjx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msidx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mcomb_sim_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlifetime_weighted_edge_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlifetimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlifetimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mjx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;31m#         comb_sim_mat[i,j] = 1 - full_weighted_edge_distance(sample_graphs[ix],sample_graphs[jx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3a6a0b0e0c1e>\u001b[0m in \u001b[0;36mlifetime_weighted_edge_distance\u001b[0;34m(subgraphs1, subgraphs2, lifetimes1, lifetimes2, ret_labels)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubgraphs1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msg1keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0medges1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlifetimes1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mml1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msg2keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubgraphs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msg2keys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# comb_sim_mat = np.zeros((len(combs),len(adv_combs)))\n",
    "# for i in range(len(combs)):\n",
    "#     g = combs[i]\n",
    "#     for j in range(len(adv_combs)):\n",
    "#         ag = adv_combs[j]\n",
    "#         comb_sim_mat[i,j] = 1 - weighted_edge_distance(g,ag)\n",
    "\n",
    "# comb_sim_mat = np.zeros((len(sample_graphs), len(adv_sample_graphs)))\n",
    "# for i in range(len(sample_graphs)):\n",
    "#     for j in range(len(adv_sample_graphs)):\n",
    "#         comb_sim_mat[i,j] = 1 - lifetime_weighted_edge_distance(sample_graphs[i],adv_sample_graphs[j],lifetimes[i],adv_lifetimes[j])\n",
    "\n",
    "comb_sim_mat = np.zeros((len(sample_graphs), len(sample_graphs)))\n",
    "sidx = list(sorted_res_df.index)\n",
    "for i in range(len(sample_graphs)):\n",
    "    ix = sidx[i]\n",
    "    for j in range(len(sample_graphs)):\n",
    "        jx = sidx[j]\n",
    "        comb_sim_mat[i,j] = 1 - lifetime_weighted_edge_distance(sample_graphs[ix],sample_graphs[jx],lifetimes[ix],lifetimes[jx])\n",
    "#         comb_sim_mat[i,j] = 1 - full_weighted_edge_distance(sample_graphs[ix],sample_graphs[jx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csmdf = pd.DataFrame(comb_sim_mat, columns=sorted_res_df['prediction'], index=sorted_res_df['prediction'])\n",
    "# ax = sns.heatmap(comb_sim_mat, xticklabels=sorted_res_df['prediction'],yticklabels=sorted_res_df['prediction'])\n",
    "ax = sns.heatmap(csmdf, xticklabels=10, yticklabels=10)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tick_params(axis='y',labelsize=5)\n",
    "plt.tick_params(axis='x',labelsize=5)\n",
    "plt.ylabel('Unaltered Subgraphs')\n",
    "plt.xlabel('Unaltered Subgraphs')\n",
    "plt.savefig('/home/tgebhart/projects/pt_activation/logdir/vis/ccff_fashion/comb_heatmap_no_adv.png', format='png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_nns = np.empty(nns.shape)\n",
    "sidxs = list(sorted_res_df.index)\n",
    "for i in range(len(sidxs)):\n",
    "    for j in range(len(sidxs)):\n",
    "        sorted_nns[i,j] = nns[sidxs[i],sidxs[j]]\n",
    "        \n",
    "snnsdf = pd.DataFrame(sorted_nns, columns=sorted_res_df['prediction'], index=sorted_res_df['prediction'])\n",
    "ax = sns.heatmap(snnsdf, xticklabels=10, yticklabels=10)\n",
    "plt.yticks(rotation=0)\n",
    "plt.xticks(rotation=0)\n",
    "plt.tick_params(axis='y',labelsize=5)\n",
    "plt.tick_params(axis='x',labelsize=5)\n",
    "plt.ylabel('Unaltered Images')\n",
    "plt.xlabel('Unaltered Images')\n",
    "plt.savefig('/home/tgebhart/projects/pt_activation/logdir/vis/ccff_fashion/nnsim.png', format='png', dpi=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
