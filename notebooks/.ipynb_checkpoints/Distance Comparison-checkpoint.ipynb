{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import parse\n",
    "import pickle\n",
    "import copy\n",
    "import math\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import dionysus as dion\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import manifold\n",
    "from collections import OrderedDict\n",
    "import sklearn\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "from pt_activation.models.fff import FFF as FFFRelu\n",
    "from pt_activation.models.linear import FFF\n",
    "from pt_activation.models.simple_mnist import CFF as CFFRelu\n",
    "from pt_activation.models.simple_mnist_sigmoid import CFF as CFFSigmoid\n",
    "from pt_activation.models.ccff import CCFF as CCFFRelu\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_info(filename):\n",
    "    format_string = 'true-{}_adv-{}_sample-{}.npy'\n",
    "    parsed = parse.parse(format_string, filename)\n",
    "    return {'true class':int(parsed[0]), 'adv class':int(parsed[1]), 'sample':int(parsed[2])}\n",
    "\n",
    "def read_adversaries(loc):\n",
    "    ret = []\n",
    "    for f in os.listdir(loc):\n",
    "        if os.path.isfile(os.path.join(loc,f)) and f.find('.npy') != -1:\n",
    "            adv = np.load(os.path.join(loc, f))\n",
    "            info = get_adv_info(f)\n",
    "            info['adversary'] = adv\n",
    "            ret.append(info)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def create_diagram(f):\n",
    "    m = dion.homology_persistence(f)\n",
    "    dgms = dion.init_diagrams(m,f)\n",
    "    return dgms[0]\n",
    "\n",
    "\n",
    "def create_lifetimes(dgms):\n",
    "    return [[pt.birth - pt.death for pt in dgm if pt.death < np.inf] for dgm in dgms]\n",
    "\n",
    "def get_example_images(test_loader):\n",
    "    ret = {}\n",
    "    for data, target in test_loader:\n",
    "        if target.numpy()[0] not in ret:\n",
    "            ret[target.numpy()[0]] = data.numpy()\n",
    "    return ret\n",
    "\n",
    "def create_diagrams(model, batch_size, up_to, test_loader, filtration=True):\n",
    "    device = torch.device(\"cpu\")\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.FashionMNIST('../../data/fashion', train=False, download=True, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])), batch_size=1, shuffle=False, **kwargs)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    images = []\n",
    "    diagrams = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss = F.nll_loss(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for s in range(data.shape[0]):\n",
    "                this_hiddens = [hiddens[i][s] for i in range(len(hiddens))]\n",
    "                print('Filtration: {}'.format(s+t))\n",
    "                if filtration:\n",
    "                    f = model.compute_dynamic_filtration(data[s], this_hiddens)\n",
    "                    dg = create_diagram(f)\n",
    "                    diagrams.append(dg)\n",
    "                images.append(data.cpu().numpy())\n",
    "                row = {'loss':test_loss, 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0]}\n",
    "                res_df.append(row)\n",
    "\n",
    "\n",
    "            t += batch_size\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df), subgraphs, diagrams\n",
    "\n",
    "\n",
    "def create_adversary_diagrams(model, batch_size, up_to, adversaries):\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    adv_images = torch.tensor(np.array([a['adversary'] for a in adversaries]))\n",
    "    adv_labels = torch.tensor(np.array([a['true class'] for a in adversaries]))\n",
    "    adv_samples = [a['sample'] for a in adversaries]\n",
    "\n",
    "    print(adv_images.shape, adv_labels.shape)\n",
    "\n",
    "    advs = torch.utils.data.TensorDataset(adv_images, adv_labels)\n",
    "    test_loader = torch.utils.data.DataLoader(advs, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    t = 0\n",
    "    res_df = []\n",
    "    diagrams = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, hiddens = model(data, hiddens=True)\n",
    "            test_loss = F.nll_loss(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            for s in range(data.shape[0]):\n",
    "                this_hiddens = [hiddens[i][s] for i in range(len(hiddens))]\n",
    "                print('Filtration: {}'.format(s+t))\n",
    "                if filtration:\n",
    "                    f = model.compute_dynamic_filtration(data[s], this_hiddens)\n",
    "                    sg, dg = create_sample_graph(f)\n",
    "                    subgraphs.append(sg)\n",
    "                    diagrams.append(dg)\n",
    "                row = {'loss':test_loss, 'class':target.cpu().numpy()[s], 'prediction':pred.cpu().numpy()[s][0], 'sample':adv_samples[t]}\n",
    "                res_df.append(row)\n",
    "\n",
    "\n",
    "            t += (batch_size)\n",
    "            if t >= up_to:\n",
    "                break\n",
    "\n",
    "    return pd.DataFrame(res_df), subgraphs, diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_directory_loc = '/home/tgebhart/projects/pt_activation/logdir/adversaries/fashion/carliniwagnerl2/cff_relu.pt'\n",
    "adversaries = read_adversaries(adv_directory_loc)\n",
    "adversaries = sorted(adversaries,  key=lambda k: k['sample'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
